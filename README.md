<h1>ğŸ“Š Telecom Customer Churn Prediction</h1>
<h2>ğŸ“Œ Project Overview</h2>

Customer churn is one of the biggest challenges in the telecom industry. Retaining existing customers is significantly more cost-effective than acquiring new ones.

This project builds a Machine Learning pipeline to predict whether a telecom customer will churn based on demographic, behavioral, and subscription-related features.

The final solution is deployed using Streamlit with SHAP-based explainability to provide transparent predictions.

<h2>ğŸ¯ Business Problem</h2>

Telecom companies face revenue loss due to customer churn.

The objective of this project is:

To accurately predict whether a customer will churn so that the company can take proactive retention actions.

<h2>ğŸ“‚ Dataset Information</h2>

ğŸ“Š Total Rows: 505,207

ğŸ“Œ Total Features: 12

Key Features:

Age

Gender

Tenure

Usage Frequency

Support Calls

Payment Delay

Subscription Type (Basic, Standard, Premium)

Contract Length (Monthly, Quarterly, Annual)

Total Spend

Last Interaction

Target Variable: Churn (0/1)

<h2>ğŸ” Exploratory Data Analysis (EDA) Insights</h2>

Key business insights discovered:

ğŸ“Œ Customers with Monthly contracts have higher churn rate.

ğŸ“Œ Customers with more than 4 support calls show strong churn tendency.

ğŸ“Œ Payment delays greater than 20 days significantly increase churn probability.

ğŸ“Œ Customers with Total Spend < 500 are more likely to churn.

ğŸ“Œ Customers above 50 years have higher churn probability.

<h2>ğŸ›  Tech Stack</h2>

Python

Pandas

NumPy

Matplotlib & Seaborn

Scikit-learn

XGBoost

LightGBM

Optuna (Hyperparameter Tuning)

SHAP (Explainable AI)

Streamlit (Deployment)

<h2>âš™ï¸ Data Preprocessing</h2>

Dropped CustomerID

Removed null values

Converted float features to integer

Used ColumnTransformer

OneHotEncoder â†’ Gender

OrdinalEncoder â†’ Subscription Type & Contract Length

Pipeline used for clean preprocessing + modeling

<h2>ğŸ¤– Model Selection</h2>

The following models were evaluated using cross-validation:

Logistic Regression

Decision Tree

Random Forest

XGBoost

LightGBM

<h2>ğŸ“ˆ Best Performing Models:</h2>

Random Forest

LightGBM

<h2>ğŸ”§ Hyperparameter Tuning</h2>

Hyperparameter tuning was performed using Optuna for:

Random Forest

LightGBM

Both models showed comparable performance with similar:

Accuracy

Precision

Recall

F1-score

<h2>ğŸ“Š Model Evaluation</h2>

Evaluation Metrics Used:

Accuracy

Precision

Recall

F1-Score

Classification Report

Both RandomForest and LightGBM achieved strong and balanced performance on the test set.

<h2>ğŸš€ Streamlit Web App</h2>

An interactive web application was built using Streamlit where users can:

Enter customer details

Get churn prediction

View churn probability

See SHAP Waterfall explanation for transparency

<h2>ğŸ” Features of the App:</h2>

âœ” Real-time prediction
âœ” Probability score
âœ” SHAP explainability
âœ” Clean UI

<h2>ğŸ§  Explainable AI (SHAP)</h2>

To ensure model transparency:

Used shap.TreeExplainer

Generated SHAP Waterfall plots

Identified top contributing features for each prediction

This makes the model production-ready and trustworthy.

<h2>ğŸ“ Project Structure</h2>
Customer-Churn-Prediction/
â”‚
â”œâ”€â”€ CustomerChurn.ipynb
â”œâ”€â”€ app.py
â”œâ”€â”€ model.pkl
â”œâ”€â”€ mydata.csv
â””â”€â”€ README.md

<h2>ğŸ’» How to Run Locally</h2>
<h2>1ï¸âƒ£ Clone the repository</h2>
git clone https://github.com/rajjaiswal159/Telecom-Customer-Churn-Prediction.git
cd customer-churn-prediction

<h2>2ï¸âƒ£ Install dependencies</h2>
pip install -r requirements.txt

<h2>3ï¸âƒ£ Run Streamlit app</h2>
streamlit run app.py

<h2>ğŸ“Œ Key Learning Outcomes</h2>

End-to-end ML Pipeline creation

Feature engineering & preprocessing

Hyperparameter tuning with Optuna

Model comparison & evaluation

Explainable AI using SHAP

Model deployment using Streamlit

<h2>ğŸ‘¨â€ğŸ’» Author</h2>

Raj Jaiswal
B.Tech (Computer Science & Engineering)
Aspiring Data Scientist

â­ If you found this project useful, consider giving it a star!
